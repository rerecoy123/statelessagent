# SAME — Stateless Agent Memory Engine

> Every AI session starts from zero. **Not anymore.**

Give your AI coding agent persistent memory — local, automatic, private.

## Install

```bash
curl -fsSL statelessagent.com/install.sh | bash
```

Requires [Ollama](https://ollama.ai) for local embeddings.

## What it does

- **Surfaces context automatically** — your prompt gets matched to relevant notes and injected as context. No copy-pasting.
- **Extracts decisions and generates handoffs** — decisions get logged, session summaries get created, so the next session picks up where you left off.
- **Runs entirely on your machine** — Ollama embeddings + SQLite. No cloud, no API keys, no accounts.

## How it works

```
Your Notes  →  Ollama  →  SQLite  →  Agent Remembers
  (.md)       (embed)    (search)    (hooks / MCP)
```

## Quick start

```bash
cd ~/my-notes
same init
```

One command checks Ollama, finds your notes, indexes them, generates config, and sets up integrations.

## Works with

| Tool | Integration |
|------|-------------|
| **Claude Code** | Hooks + MCP |
| **Cursor** | MCP |
| **Windsurf** | MCP |
| **Obsidian** | Vault detection |
| **Logseq** | Vault detection |
| **Any MCP client** | 6 tools |

SAME works with any directory of `.md` files. No Obsidian required.

Use `same init --mcp-only` to skip Claude Code hooks and just register the MCP server.

## Built with

Go · SQLite + sqlite-vec · Ollama

<details>
<summary><strong>CLI Reference</strong></summary>

| Command | Description |
|---------|-------------|
| `same init` | Interactive setup wizard |
| `same status` | At-a-glance system status |
| `same log` | Recent SAME activity |
| `same search <query>` | Search from the command line |
| `same related <path>` | Find similar notes |
| `same reindex [--force]` | Re-index markdown files |
| `same doctor` | System health check with fix suggestions |
| `same config show` | Show effective configuration |
| `same config edit` | Open config in $EDITOR |
| `same setup hooks` | Install/update Claude Code hooks |
| `same setup mcp` | Register MCP server |
| `same stats` | Index statistics |
| `same watch` | Auto-reindex on file changes |
| `same budget` | Context utilization report |
| `same vault list\|add\|remove` | Manage multiple vaults |
| `same version [--check]` | Version and update check |

</details>

<details>
<summary><strong>Configuration</strong></summary>

SAME uses `.same/config.toml`, generated by `same init`:

```toml
[vault]
path = "/Users/sean/Documents/notes"
# skip_dirs = [".venv", "build"]
handoff_dir = "sessions"
decision_log = "decisions.md"

[ollama]
url = "http://localhost:11434"
model = "nomic-embed-text"

[memory]
max_token_budget = 800
max_results = 2
distance_threshold = 16.2
composite_threshold = 0.65

[hooks]
context_surfacing = true
decision_extractor = true
handoff_generator = true
staleness_check = true
```

Configuration priority (highest wins):

1. CLI flags (`--vault`)
2. Environment variables (`VAULT_PATH`, `OLLAMA_URL`, `SAME_*`)
3. Config file (`.same/config.toml`)
4. Built-in defaults

| Variable | Default | Description |
|----------|---------|-------------|
| `VAULT_PATH` | auto-detect | Path to your markdown notes |
| `OLLAMA_URL` | `http://localhost:11434` | Ollama API (must be localhost) |
| `SAME_DATA_DIR` | `<vault>/.same/data` | Database location |
| `SAME_HANDOFF_DIR` | `sessions` | Handoff notes directory |
| `SAME_DECISION_LOG` | `decisions.md` | Decision log path |
| `SAME_SKIP_DIRS` | *(none)* | Extra dirs to skip (comma-separated) |

</details>

<details>
<summary><strong>MCP Server</strong></summary>

SAME exposes 6 tools via MCP:

| Tool | Description |
|------|-------------|
| `search_notes` | Semantic search |
| `search_notes_filtered` | Search with domain/workstream/tag filters |
| `get_note` | Read full note by path |
| `find_similar_notes` | Find related notes |
| `reindex` | Re-index the vault |
| `index_stats` | Index statistics |

</details>

## FAQ

**Do I need Obsidian?**
No. Any directory of `.md` files works.

**Does it slow down my prompts?**
50-200ms. Ollama embedding is the bottleneck — search and scoring take <5ms.

**Is my data sent anywhere?**
SAME is fully local. Context injected into your AI tool is sent to that tool's API as part of your conversation, same as pasting it manually.

**How much disk space?**
5-15MB for a few hundred notes.

**Can I use multiple vaults?**
Yes. `same vault add work ~/work-notes && same vault default work`.

## Security & Privacy

- All data stays local — no external API calls except Ollama on localhost
- Ollama URL validated to localhost-only
- `_PRIVATE/` directories excluded from indexing and context surfacing
- Snippets scanned for prompt injection patterns before injection
- Path traversal blocked in MCP `get_note` tool

## Building from Source

```bash
git clone https://github.com/sgx-labs/statelessagent.git
cd statelessagent && make install
```

Requires Go 1.23+ and CGO.

## Support

[Buy me a coffee](https://buymeacoffee.com/sgxlabs) · [GitHub Sponsors](https://github.com/sponsors/sgx-labs)

## License

BSL 1.1 — free for personal, educational, hobby, research, and evaluation use. Change date: 2030-02-02 (converts to Apache 2.0). See [LICENSE](LICENSE).
